<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
            "http://www.w3.org/TR/REC-html40/loose.dtd">
<HTML>
<HEAD>

<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<META name="GENERATOR" content="hevea 1.10">

<META name="Author" content="Julien Mairal">
<link rel="stylesheet" href="doc_spams.css">
<LINK rel="stylesheet" type="text/css" href="doc_spams.css">
<TITLE>References</TITLE>
</HEAD>
<BODY >
<A HREF="doc_spams008.html"><IMG SRC="previous_motif.gif" ALT="Previous"></A>
<A HREF="index.html"><IMG SRC="contents_motif.gif" ALT="Up"></A>
<HR>
<H2 CLASS="section">References</H2><DL CLASS="thebibliography"><DT CLASS="dt-thebibliography">
<A NAME="beck"><FONT COLOR=purple>[1]</FONT></A></DT><DD CLASS="dd-thebibliography">
A. Beck and M. Teboulle.
A fast iterative shrinkage-thresholding algorithm for linear inverse
problems.
<EM>SIAM Journal on Imaging Sciences</EM>, 2(1):183–202, 2009.</DD><DT CLASS="dt-thebibliography"><A NAME="borwein"><FONT COLOR=purple>[2]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. M. Borwein and A. S. Lewis.
<EM>Convex analysis and nonlinear optimization: Theory and
examples</EM>.
Springer, 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="brucker"><FONT COLOR=purple>[3]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. Brucker.
An O(n) algorithm for quadratic knapsack problems.
3:163–166, 1984.</DD><DT CLASS="dt-thebibliography"><A NAME="candes4"><FONT COLOR=purple>[4]</FONT></A></DT><DD CLASS="dd-thebibliography">
E. J. Candès, M. Wakin, and S. Boyd.
Enhancing sparsity by reweighted l1 minimization.
<EM>Journal of Fourier Analysis and Applications</EM>, 14:877–905,
2008.</DD><DT CLASS="dt-thebibliography"><A NAME="cherkassky"><FONT COLOR=purple>[5]</FONT></A></DT><DD CLASS="dd-thebibliography">
B. V. Cherkassky and A. V. Goldberg.
On implementing the push-relabel method for the maximum flow problem.
<EM>Algorithmica</EM>, 19(4):390–410, 1997.</DD><DT CLASS="dt-thebibliography"><A NAME="cotter"><FONT COLOR=purple>[6]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. F. Cotter, J. Adler, B. Rao, and K. Kreutz-Delgado.
Forward sequential algorithms for best basis selection.
In <EM>IEEE Proceedings of Vision Image and Signal Processing</EM>,
pages 235–244, 1999.</DD><DT CLASS="dt-thebibliography"><A NAME="duchi"><FONT COLOR=purple>[7]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
Efficient projections onto the ℓ<SUB>1</SUB>-ball for learning in high
dimensions.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2008.</DD><DT CLASS="dt-thebibliography"><A NAME="efron"><FONT COLOR=purple>[8]</FONT></A></DT><DD CLASS="dd-thebibliography">
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression.
<EM>Annals of statistics</EM>, 32(2):407–499, 2004.</DD><DT CLASS="dt-thebibliography"><A NAME="friedman"><FONT COLOR=purple>[9]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Friedman, T. Hastie, H. Hölfling, and R. Tibshirani.
Pathwise coordinate optimization.
<EM>Annals of statistics</EM>, 1(2):302–332, 2007.</DD><DT CLASS="dt-thebibliography"><A NAME="fu"><FONT COLOR=purple>[11]</FONT></A></DT><DD CLASS="dd-thebibliography">
W. J. Fu.
Penalized regressions: The bridge versus the Lasso.
<EM>Journal of computational and graphical statistics</EM>, 7:397–416,
1998.</DD><DT CLASS="dt-thebibliography"><A NAME="goldberg"><FONT COLOR=purple>[12]</FONT></A></DT><DD CLASS="dd-thebibliography">
A. V. Goldberg and R. E. Tarjan.
A new approach to the maximum flow problem.
In <EM>Proc. of ACM Symposium on Theory of Computing</EM>, pages
136–146, 1986.</DD><DT CLASS="dt-thebibliography"><A NAME="hoyer"><FONT COLOR=purple>[13]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. O. Hoyer.
Non-negative sparse coding.
In <EM>Proc. IEEE Workshop on Neural Networks for Signal
Processing</EM>, 2002.</DD><DT CLASS="dt-thebibliography"><A NAME="jenatton3"><FONT COLOR=purple>[14]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for sparse hierarchical dictionary learning.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="jenatton4"><FONT COLOR=purple>[15]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for hierarchical sparse coding.
<EM>Journal of Machine Learning Research</EM>, 12:2297–2334, 2011.</DD><DT CLASS="dt-thebibliography"><A NAME="lee2"><FONT COLOR=purple>[16]</FONT></A></DT><DD CLASS="dd-thebibliography">
D. D. Lee and H. S. Seung.
Algorithms for non-negative matrix factorization.
In <EM>Advances in Neural Information Processing Systems</EM>, 2001.</DD><DT CLASS="dt-thebibliography"><A NAME="maculan"><FONT COLOR=purple>[17]</FONT></A></DT><DD CLASS="dd-thebibliography">
N. Maculan and J. R. G. Galdino de Paula.
A linear-time median-finding algorithm for projecting a vector on the
simplex of Rn.
<EM>Operations research letters</EM>, 8(4):219–222, 1989.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal11"><FONT COLOR=purple>[18]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal.
<EM>Sparse coding for machine learning, image processing and
computer vision</EM>.
PhD thesis, Ecole Normale Supérieure, Cachan, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal7"><FONT COLOR=purple>[19]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online dictionary learning for sparse coding.
In <EM>Proceedings of the International Conference on Machine
Learning (ICML)</EM>, 2009.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal9"><FONT COLOR=purple>[20]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online learning for matrix factorization and sparse coding.
<EM>Journal of Machine Learning Research</EM>, 11:19–60, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mairal10"><FONT COLOR=purple>[21]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach.
Network flow algorithms for structured sparsity.
In <EM>Advances in Neural Information Processing Systems</EM>, 2010.</DD><DT CLASS="dt-thebibliography"><A NAME="mallat4"><FONT COLOR=purple>[24]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. Mallat and Z. Zhang.
Matching pursuit in a time-frequency dictionary.
<EM>IEEE Transactions on Signal Processing</EM>, 41(12):3397–3415,
1993.</DD><DT CLASS="dt-thebibliography"><A NAME="meinshausen"><FONT COLOR=purple>[25]</FONT></A></DT><DD CLASS="dd-thebibliography">
N. Meinshausen and P. Buehlmann.
Stability selection.
Technical report.
ArXiv:0809.2932.</DD><DT CLASS="dt-thebibliography"><A NAME="obozinski"><FONT COLOR=purple>[26]</FONT></A></DT><DD CLASS="dd-thebibliography">
G. Obozinski, B. Taskar, and M.I. Jordan.
Joint covariate selection and joint subspace selection for multiple
classification problems.
<EM>Statistics and Computing</EM>, pages 1–22.</DD><DT CLASS="dt-thebibliography"><A NAME="sprechmann"><FONT COLOR=purple>[28]</FONT></A></DT><DD CLASS="dd-thebibliography">
P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar.
Collaborative hierarchical sparse modeling.
Technical report, 2010.
Preprint arXiv:1003.0400v1.</DD><DT CLASS="dt-thebibliography"><A NAME="tibshirani2"><FONT COLOR=purple>[29]</FONT></A></DT><DD CLASS="dd-thebibliography">
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight.
Sparsity and smoothness via the fused lasso.
<EM>Journal of the Royal Statistical Society Series B</EM>,
67(1):91–108, 2005.</DD><DT CLASS="dt-thebibliography"><A NAME="tropp3"><FONT COLOR=purple>[30]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. A. Tropp.
Algorithms for simultaneous sparse approximation. part ii: Convex
relaxation.
<EM>Signal Processing, special issue "Sparse approximations in
signal and image processing"</EM>, 86:589–602, April 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="tropp2"><FONT COLOR=purple>[31]</FONT></A></DT><DD CLASS="dd-thebibliography">
J. A. Tropp, A. C. Gilbert, and M. J. Strauss.
Algorithms for simultaneous sparse approximation. part i: Greedy
pursuit.
<EM>Signal Processing, special issue "sparse approximations in
signal and image processing"</EM>, 86:572–588, April 2006.</DD><DT CLASS="dt-thebibliography"><A NAME="weisberg"><FONT COLOR=purple>[32]</FONT></A></DT><DD CLASS="dd-thebibliography">
S. Weisberg.
<EM>Applied Linear Regression</EM>.
Wiley, New York, 1980.</DD><DT CLASS="dt-thebibliography"><A NAME="wu"><FONT COLOR=purple>[33]</FONT></A></DT><DD CLASS="dd-thebibliography">
T. T. Wu and K. Lange.
Coordinate descent algorithms for Lasso penalized regression.
<EM>Annals of Applied Statistics</EM>, 2(1):224–244, 2008.</DD><DT CLASS="dt-thebibliography"><A NAME="yuan"><FONT COLOR=purple>[34]</FONT></A></DT><DD CLASS="dd-thebibliography">
M. Yuan and Y. Lin.
Model selection and estimation in regression with grouped variables.
<EM>Journal of the Royal Statistical Society Series B</EM>, 68:49–67,
2006.</DD><DT CLASS="dt-thebibliography"><A NAME="zou"><FONT COLOR=purple>[35]</FONT></A></DT><DD CLASS="dd-thebibliography">
H. Zou and T. Hastie.
Regularization and variable selection via the elastic net.
<EM>Journal of the Royal Statistical Society Series B</EM>,
67(2):301–320, 2005.</DD></DL><HR>
<A HREF="doc_spams008.html"><IMG SRC="previous_motif.gif" ALT="Previous"></A>
<A HREF="index.html"><IMG SRC="contents_motif.gif" ALT="Up"></A>
</BODY>
</HTML>
